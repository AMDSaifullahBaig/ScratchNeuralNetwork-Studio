{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c0056e",
   "metadata": {},
   "source": [
    "# Modular Neural Network Main File\n",
    "**Author:** MD Saifullah Baig.A\n",
    "<br>\n",
    "**Version:** 2.0 (Vectorized and Mini-Batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c74dfc",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies\n",
    "We import standard libraries for mathematics (`numpy`) and visualization (`matplotlib`). \n",
    "Crucially, we import our custom `Neural_Network_Engine`, which contains the `Neural_Network`, `Connected_Layers`, and `Activation_Layer` classes built from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "try:\n",
    "    from py_code.Neural_Network_Engine import Neural_Network,Connected_Layers,Activation_Layer,Activation\n",
    "except ImportError:\n",
    "    print(\"Warning: Neural_Network_Engine not found. Class features will not work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea773a2d",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Helper Functions\n",
    "\n",
    "### Standard Scaler (Z-Score Normalization)\n",
    "Neural networks converge faster and more stably when input features are on a similar scale. This function normalizes the data to have a **Mean ($\\mu$) of 0** and a **Standard Deviation ($\\sigma$) of 1**.\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma + \\epsilon}$$\n",
    "\n",
    "The term $\\epsilon$ (set to `1e-8` in the code) is added for **Numerical Stability**.\n",
    "\n",
    "1.  **Prevents Division by Zero:**\n",
    "    * Standard Deviation ($\\sigma$) measures the spread of data.\n",
    "    * If a feature column contains **constant values** (e.g., every row has `Age = 25`), the variance and standard deviation will be **0**.\n",
    "    * Without $\\epsilon$, the computer would attempt to divide by zero ($\\frac{0}{0}$), causing the program to crash or resulting in `NaN` (Not a Number) or `Infinity`.\n",
    "\n",
    "2.  **Safety Net:**\n",
    "    * By adding a tiny number like $0.00000001$, the denominator becomes slightly larger than zero ($0 + \\epsilon$), allowing the calculation to proceed safely even on \"flat\" data features.\n",
    "\n",
    "**Returns:**\n",
    "* `scaled`: The normalized data.\n",
    "* `mean`, `std`: Stored to inverse-transform predictions later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d869ebd",
   "metadata": {},
   "source": [
    "### ❓ Why is Scaling Necessary?\n",
    "\n",
    "In Deep Learning, **Standard Scaling** is not optional—it is mathematically critical for the network to learn effectively. Here are the three main reasons why:\n",
    "\n",
    "#### 1. Prevents Feature Dominance (\"Apples vs. Oranges\")\n",
    "Neural networks use matrix multiplication (`Input * Weight`). If features have vastly different ranges, the larger numbers will dominate the learning process.\n",
    "* **Example:**\n",
    "    * *BMI:* Range 18–35\n",
    "    * *Income:* Range 20,000–100,000\n",
    "* **Result without Scaling:** The network sees \"Income\" as 1000x more important than \"BMI\" simply because the number is bigger. It effectively ignores the smaller feature.\n",
    "* **With Scaling:** Both features are forced into a similar range (approx. -3 to +3), giving them equal importance.\n",
    "\n",
    "#### 2. Faster Convergence (The \"Bowl\" Shape)\n",
    "The optimizer (Gradient Descent) tries to find the lowest error.\n",
    "* **Unscaled Data:** The error surface looks like a long, narrow valley. The optimizer zig-zags back and forth, taking a long time to reach the bottom.\n",
    "* **Scaled Data:** The error surface looks like a symmetrical bowl. The optimizer can take a direct path to the minimum, reducing training time significantly.\n",
    "\n",
    "#### 3. Avoids Vanishing Gradients (Activation Saturation)\n",
    "Activation functions like `Tanh` and `Sigmoid` are sensitive to large inputs.\n",
    "* **The Problem:** `Tanh(100)` is `1.0`. The slope (gradient) at this point is **Zero**.\n",
    "* **The Consequence:** If you feed raw large numbers (like 150) into the network, the gradients become zero immediately. The weights stop updating, and the network stops learning (Vanishing Gradient Problem).\n",
    "* **The Solution:** Scaling keeps inputs close to 0 (e.g., -1 to 1), where the activation functions have the steepest slope and strongest gradients.\n",
    "\n",
    "| Feature | Raw Data | Scaled Data |\n",
    "| :--- | :--- | :--- |\n",
    "| **Range** | Wildly different (e.g., 0.001 to 1,000,000) | Standardized (~ -3 to +3) |\n",
    "| **Training Speed** | Very Slow | Fast |\n",
    "| **Stability** | Prone to NaN / Infinity errors | Stable |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standard_Scaler(data):\n",
    "    mean=np.mean(data,axis=0)\n",
    "    std=np.std(data,axis=0)+1e-8\n",
    "    scaled=(data-mean)/std\n",
    "    return scaled,mean,std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b916ce",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "To evaluate the model fairly, we must test it on data it has never seen before.\n",
    "This function:\n",
    "1. Generates a list of indices.\n",
    "2. **Shuffles** them randomly to remove any ordering bias.\n",
    "3. Splits the data into **Training (80%)** and **Testing (20%)** sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X,Y,test_size=0.2):\n",
    "    idx=np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    split_range=int(X.shape[0]*(1-test_size))\n",
    "    train_idx,test_idx=idx[:split_range],idx[split_range:]\n",
    "    return X[train_idx],X[test_idx],Y[train_idx],Y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0184b54",
   "metadata": {},
   "source": [
    "## 3. Visualization\n",
    "This function generates two plots to evaluate performance:\n",
    "1. **Training Convergence:** Plots the MSE Loss over epochs (should decrease).\n",
    "2. **Prediction Accuracy:** A scatter plot comparing True values vs. Predicted values. A perfect model would align all points on the diagonal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848aa11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(loss_history,true,prediction):\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(loss_history,label=\"Training Loss\",color=\"blue\")\n",
    "    plt.title(\"Training Convergence\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.grid(True,linestyle=\"--\",alpha=0.6)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(true,prediction,alpha=0.6,color='red',edgecolors='k')\n",
    "\n",
    "    if len(true) > 0 and len(prediction) > 0:\n",
    "        least=min(true.min(),prediction.min())\n",
    "        highest=max(true.max(),prediction.max())\n",
    "        plt.plot([least,highest],[least,highest],'k--',lw=2,label=\"Perfect Fit\")\n",
    "\n",
    "    plt.title(\"True VS Predicted Values\")\n",
    "    plt.xlabel(\"True labels\")\n",
    "    plt.ylabel(\"Predicted Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True,linestyle='--',alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1641451b",
   "metadata": {},
   "source": [
    "## 4. The Backend Controller (Bridge to GUI)\n",
    "\n",
    "The `Neural_Network_Backend` class acts as the **Controller** in our GUI architecture. It separates the raw mathematics (Engine) from the user interface (GUI).\n",
    "\n",
    "**Responsibilities:**\n",
    "1.  **State Management:** Keeps track of the layer stack (`layer_stack`) and dataset (`meta_data`) before the model is actually built.\n",
    "2.  **Data Pipeline:** Handles loading datasets (XOR, Diabetes) and applying **Standard Scaling** automatically.\n",
    "3.  **Model Building:** Converts the user's \"Layer Config\" list into actual `Connected_Layers` and `Activation_Layers` objects.\n",
    "4.  **Training Bridge:** Runs the training loop on a background thread (via the GUI) and passes the **Mini-Batch** configuration (`batch_size=10`) to the Engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ce7ce",
   "metadata": {},
   "source": [
    "### `__init__`\n",
    "Initializes the backend controller.\n",
    "* **`layer_stack`**: A list to temporarily hold layer configurations (dictionaries) before the model is built.\n",
    "* **`loss_history`**: Stores the training loss curve.\n",
    "* **`meta_data`**: A dictionary to store the dataset ($X$, $Y$) and scaling parameters ($\\mu$, $\\sigma$) for inverse transformation.\n",
    "* **`model`**: The actual `Neural_Network` engine instance (initially `None`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network_Backend:\n",
    "    def __init__(self):\n",
    "        self.layer_stack=[]\n",
    "        self.loss_history=[]\n",
    "        self.meta_data={}\n",
    "        self.model=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983d3b0d",
   "metadata": {},
   "source": [
    "### `reset`\n",
    "Clears all internal state variables to their default values. This is crucial when the user clicks \"Reset\" in the GUI to ensure no old data or layer configurations persist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d3ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(self):\n",
    "    self.layer_stack=[]\n",
    "    self.loss_history=[]\n",
    "    self.meta_data={}\n",
    "    self.model=None\n",
    "Neural_Network_Backend.reset=reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04239d",
   "metadata": {},
   "source": [
    "### `load_data`\n",
    "This method is responsible for loading and preprocessing the dataset selected by the user.\n",
    "\n",
    "**Functionality:**\n",
    "* **XOR Dataset:** Manually creates the non-linear \"Exclusive OR\" logic gate data (4 samples). This is perfect for debugging because a simple linear model cannot solve it.\n",
    "* **Diabetes Dataset:** Loads a real-world regression dataset from `sklearn`.\n",
    "* **Metadata Storage:** Saves the processed data and the scaling parameters (`mean`, `deviation`) so we can inverse-transform the predictions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(self,dataset=\"XOR\"):\n",
    "    match(dataset):\n",
    "        case \"XOR\":\n",
    "            X=np.array([\n",
    "                    [0,0],\n",
    "                    [0,1],\n",
    "                    [1,0],\n",
    "                    [1,1]\n",
    "              ])\n",
    "            Y=np.array([\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0]  \n",
    "              ])\n",
    "            self.meta_data={\"X_train\":X,\"X_test\":X,\"Y_train\":Y,\"Y_test\":Y,\"mean\":0.0,\"deviation\":1.0}\n",
    "        case \"Diabetes\":\n",
    "            diabetes=load_diabetes()\n",
    "            X_raw=diabetes.data\n",
    "            y_raw=diabetes.target.reshape(-1, 1)\n",
    "    \n",
    "            X_scaled,mean_x,std_x=Standard_Scaler(X_raw)\n",
    "            y_scaled,mean_y,std_y=Standard_Scaler(y_raw)\n",
    "\n",
    "            X_train,X_test,Y_train,Y_test=train_test_split(X_scaled,y_scaled,test_size=0.2)\n",
    "            self.meta_data={\"X_train\":X_train,\"X_test\":X_test,\"Y_train\":Y_train,\"Y_test\":Y_test,\"mean\":mean_y,\"deviation\":std_y}\n",
    "Neural_Network_Backend.load_data=load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8946c595",
   "metadata": {},
   "source": [
    "### `add_layer_configuration`\n",
    "This method acts as a \"staging area\" for the network architecture. Instead of creating layers immediately, it saves the user's intent into a list (`layer_stack`).\n",
    "\n",
    "**Why do we do this?**\n",
    "* **Flexibility:** It allows the user to add, remove (`pop_layer`), or clear the stack in the GUI *before* the model is actually built.\n",
    "* **Arguments:**\n",
    "    * `layer_type`: Accepts `\"L\"` for Dense layers or `\"A\"` for Activation layers.\n",
    "    * `**kwargs`: Captures dynamic parameters like `input` size, `output` size, `optimizer`, and `activation` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff375e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_layer_configuration(self,layer_type,**kwargs):\n",
    "    if layer_type==\"L\":\n",
    "        self.layer_stack.append({\n",
    "                                        \"type\":\"L\",\n",
    "                                        \"input\":kwargs.get(\"input\"),\n",
    "                                        \"output\":kwargs.get(\"output\"),\n",
    "                                        \"optimizer\":kwargs.get(\"optimizer\",\"sgd\"),\n",
    "                                        \"initializer\":kwargs.get(\"initializer\",\"xavier\"),\n",
    "                                })\n",
    "    elif layer_type==\"A\":\n",
    "        self.layer_stack.append({\n",
    "                                        \"type\":\"A\",\n",
    "                                        \"activation\":kwargs.get(\"activation\")\n",
    "                                })\n",
    "Neural_Network_Backend.add_layer_configuration=add_layer_configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46857f68",
   "metadata": {},
   "source": [
    "### `pop_layer`\n",
    "Removes the most recently added layer configuration from the stack. Used for the \"Undo\" or \"Remove Last\" button in the GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2169378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_layer(self):\n",
    "    if self.layer_stack:\n",
    "        self.layer_stack.pop()\n",
    "Neural_Network_Backend.pop_layer=pop_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c0fe72",
   "metadata": {},
   "source": [
    "### `build_model`\n",
    "Constructs the actual `Neural_Network` engine instance from the stored configuration.\n",
    "* **Process:**\n",
    "    1.  Iterates through `self.layer_stack`.\n",
    "    2.  Instantiates `Connected_Layers` or `Activation_Layer` objects based on the config.\n",
    "    3.  Adds them to a new `Neural_Network` instance.\n",
    "    4.  Saves the result to `self.model`.\n",
    "* **Arguments:** `lr` (float) - The learning rate to apply to all learnable layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f1a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(self,lr):\n",
    "    model=Neural_Network()\n",
    "    for layer in self.layer_stack:\n",
    "        if layer[\"type\"]==\"L\":\n",
    "            layer=Connected_Layers(layer[\"input\"],layer[\"output\"],learning_rate=lr)\n",
    "            model.Add(layer)\n",
    "        elif layer[\"type\"]==\"A\":\n",
    "            model.Add(Activation_Layer(layer[\"activation\"]))\n",
    "    self.model=model\n",
    "Neural_Network_Backend.build_model=build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ec72dc",
   "metadata": {},
   "source": [
    "### `train_loop`\n",
    "The main bridge between the GUI and the Engine.\n",
    "* **Functionality:**\n",
    "    1.  Retrieves training data ($X_{train}, Y_{train}$) from metadata.\n",
    "    2.  Defines a **Bridge Callback** (`epoch_complete`) that:\n",
    "        * Updates the GUI via the provided `callback`.\n",
    "        * Checks `self.stop_training` to interrupt the engine if the \"Stop\" button is pressed.\n",
    "    3.  Calls the Engine's `Training_model` method using **Mini-Batch Gradient Descent** (`batch_size=10`).\n",
    "* **Returns:** Success message or Error string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c520e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(self,epoch=1000,callback=None):\n",
    "    if not self.model or \"X_train\" not in self.meta_data:\n",
    "        return \"Error: Setup Incomplete\"\n",
    "    X=self.meta_data[\"X_train\"]\n",
    "    Y=self.meta_data[\"Y_train\"]\n",
    "    self.stop_training=False\n",
    "    def epoch_complete(curr_epoch,loss):\n",
    "        if callback and (curr_epoch%10==0 or curr_epoch==epoch-1):\n",
    "            callback(curr_epoch,loss)\n",
    "        if self.stop_training:\n",
    "            return True\n",
    "        return False\n",
    "    try:\n",
    "        self.model.Training_model(X,Y,epoch,callback=epoch_complete,batch_size=10)\n",
    "    except Exception as e:\n",
    "        return f\"Error:{e}\"\n",
    "Neural_Network_Backend.train_loop=train_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c8000c",
   "metadata": {},
   "source": [
    "### `get_result`\n",
    "Generates predictions and prepares data for visualization.\n",
    "* **Key Feature (Vectorization):** Uses `self.model.Predict(X_test)` to process the entire test set in one operation, which is significantly faster than looping.\n",
    "* **Inverse Scaling:**\n",
    "    * The model predicts *scaled* values (e.g., -1.5 to +1.5).\n",
    "    * This function converts them back to *real* values (e.g., $50 to $1000) using the stored `mean` and `deviation` from `load_data`.\n",
    "    * Formula: $Y_{real} = (Y_{scaled} \\times \\sigma) + \\mu$\n",
    "* **Returns:** `loss_history`, `Y_true_real`, `preds_real`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0829b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(self):\n",
    "    if not self.model: return [], [], []\n",
    "    \n",
    "    X_test=self.meta_data[\"X_test\"]\n",
    "    Y_test=self.meta_data[\"Y_test\"]\n",
    "    \n",
    "    preds_scaled=self.model.Predict(X_test)\n",
    "        \n",
    "    std_y=self.meta_data[\"deviation\"]\n",
    "    mean_y=self.meta_data[\"mean\"]\n",
    "    \n",
    "    preds_real=(preds_scaled*std_y)+mean_y\n",
    "    Y_true_real=(Y_test*std_y)+mean_y\n",
    "    \n",
    "    return self.model.loss_history,Y_true_real,preds_real\n",
    "Neural_Network_Backend.get_result=get_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
