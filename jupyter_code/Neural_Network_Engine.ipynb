{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c614bb54",
   "metadata": {},
   "source": [
    "# Modular Neural Network Engine\n",
    "**Author:** MD Saifullah Baig.A\n",
    "<br>\n",
    "**Version:** 3.0\n",
    "\n",
    "## Overview\n",
    "This notebook implements a modular, scratch-built Deep Learning framework in Python. It is designed to demystify the internal mechanics of deep learning by implementing backpropagation, optimizers (SGD, Adam), and dynamic layer management without relying on auto-differentiation libraries.\n",
    "\n",
    "**Key Features:**\n",
    "* **Modular Architecture:** Dynamic stacking of layers.\n",
    "* **Advanced Optimizers:** Custom implementation of Adam and SGD.\n",
    "* **Vectorized Operations:** High-performance matrix computations using NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65612f5a",
   "metadata": {},
   "source": [
    "## 1. Dependencies\n",
    "We utilize `NumPy` for high-performance matrix operations (Linear Algebra), which form the backbone of the tensor computations. `Pandas` and `Matplotlib` are employed for data management and visualizing the loss convergence during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c88ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd94e2",
   "metadata": {},
   "source": [
    "## 2. Activation Functions & Derivatives\n",
    "This static registry defines the non-linear activation functions ($\\phi$) and their derivatives ($\\phi'$). These functions are crucial for enabling the network to learn complex, non-linear patterns.\n",
    "\n",
    "**Implemented Mathematics:**\n",
    "* **Sigmoid:** $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "* **Tanh:** $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "* **ReLU:** $f(x) = \\max(0, x)$\n",
    "* **MSE Loss:** $L = \\frac{1}{n} \\sum (y_{true} - y_{pred})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f722e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation():\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0,x)\n",
    "    @staticmethod\n",
    "    def mse(true,predicted):\n",
    "        return np.mean((true-predicted)**2)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def del_sigmoid(x):\n",
    "        return x*(1-x)\n",
    "    @staticmethod\n",
    "    def del_tanh(x):\n",
    "        return 1 - x**2\n",
    "    @staticmethod\n",
    "    def del_relu(x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    @staticmethod\n",
    "    def del_mse(true,predicted):\n",
    "        return 2*(predicted - true)/true.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194a5160",
   "metadata": {},
   "source": [
    "## 3. Optimization Algorithms\n",
    "The `Optimizer` base class defines the interface for parameter updates. We implement two distinct strategies to minimize the loss function:\n",
    "\n",
    "### 3.1. Stochastic Gradient Descent (SGD)\n",
    "A standard approach updating weights proportional to the negative gradient.\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla J(\\theta)$$\n",
    "\n",
    "### 3.2. Adam (Adaptive Moment Estimation)\n",
    "A robust optimizer that adapts the learning rate for each parameter by estimating the first ($m_t$) and second ($v_t$) moments of the gradients. We include bias correction to counteract initialization bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47da53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimiser():\n",
    "    def update(self,parameters,gradient):\n",
    "        raise NotImplementedError\n",
    "class SGD(Optimiser):\n",
    "    def __init__(self,learning_rate=0.01):\n",
    "        self.learning_rate=learning_rate\n",
    "    def update(self,parameters,gradient):\n",
    "        return parameters-self.learning_rate*gradient\n",
    "class Adam(Optimiser):\n",
    "    def __init__(self,learning_rate=0.1,beta1=0.9,beta2=0.999,epsilon=1e-8):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.epsilon=epsilon\n",
    "        self.m=None\n",
    "        self.v=None\n",
    "        self.t=0\n",
    "    def update(self,parameters,gradient):\n",
    "        if self.m is None:\n",
    "            self.m=np.zeros_like(parameters)\n",
    "            self.v=np.zeros_like(parameters)\n",
    "        self.t+=1\n",
    "        self.m=self.m*self.beta1+(1-self.beta1)*gradient\n",
    "        self.v=self.v*self.beta2+(1-self.beta2)*(gradient**2)\n",
    "        m_corrected=self.m/(1-self.beta1**self.t)\n",
    "        v_corrected=self.v/(1-self.beta2**self.t)\n",
    "\n",
    "        return parameters - self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde188c",
   "metadata": {},
   "source": [
    "## 4. The Layer Interface\n",
    "The abstract `Layer` class enforces a strict contract for all network components. To maintain modularity, every layer must implement:\n",
    "\n",
    "* **`forward(input)`**: Computes the output tensor $Y$.\n",
    "* **`backward(output_error)`**: Computes the gradient with respect to the input ($\\frac{\\partial L}{\\partial X}$) and updates internal weights if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4b5a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input=None\n",
    "        self.output=None\n",
    "    def forward(self,input):\n",
    "        raise NotImplementedError\n",
    "    def backward(self,output_error):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96d2761",
   "metadata": {},
   "source": [
    "## 5. Fully Connected (Dense) Layer\n",
    "The core computational unit performing the affine transformation $Y = XW + B$.\n",
    "\n",
    "**Implementation Details:**\n",
    "* **Xavier Initialization:** Weights are initialized uniformly within $\\pm \\sqrt{\\frac{1}{n_{in}}}$ to maintain variance consistency across layers and prevent vanishing gradients.\n",
    "* **Independent Optimizers:** Each layer instance maintains its own optimizer states for weights and biases, ensuring correct momentum tracking for the Adam algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd705df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connected_Layers(Layer):\n",
    "    def __init__(self,input_size,output_size,learning_rate=0.01,optimizer=\"adam\"):\n",
    "        limit=np.sqrt(1/input_size)\n",
    "        self.weights=np.random.uniform(-limit,limit,(input_size,output_size))\n",
    "        self.bias=np.zeros((1,output_size))\n",
    "\n",
    "        self.optimizer_w=self._get_optimizer(optimizer,learning_rate)\n",
    "        self.optimizer_b=self._get_optimizer(optimizer,learning_rate)\n",
    "    def _get_optimizer(self,name,learning_rate):\n",
    "        if name.lower()==\"adam\":\n",
    "            return Adam(learning_rate=learning_rate)\n",
    "        elif name.lower()==\"sgd\":\n",
    "            return SGD(learning_rate=learning_rate)\n",
    "    def forward(self,input):\n",
    "        self.input=input\n",
    "        self.output=np.dot(self.input,self.weights)+self.bias\n",
    "        return self.output\n",
    "    def backward(self,output_error):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weight_gradient=np.dot(self.input.T,output_error)\n",
    "\n",
    "        self.weights=self.optimizer_w.update(self.weights,weight_gradient)\n",
    "        self.bias=self.optimizer_b.update(self.bias,np.sum(output_error, axis=0, keepdims=True))\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb199e",
   "metadata": {},
   "source": [
    "## 6. Activation Wrapper\n",
    "A pass-through layer that applies non-linearity element-wise to the input tensor.\n",
    "\n",
    "* **Forward:** Applies the function chosen from the `Activation` registry.\n",
    "* **Backward:** Applies the derivative via the Chain Rule: $\\delta_{in} = \\delta_{out} \\odot \\phi'(input)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5843fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Layer(Layer):\n",
    "    def __init__(self,activation):\n",
    "        self.activation={\n",
    "                            \"tanh\":(Activation.tanh,Activation.del_tanh),\n",
    "                            \"sigmoid\":(Activation.sigmoid,Activation.del_sigmoid),\n",
    "                            \"relu\":(Activation.relu,Activation.del_relu)\n",
    "                        }\n",
    "        self.activation_forward,self.activation_backward=self.activation[activation]\n",
    "    def forward(self,input):\n",
    "        self.input=input\n",
    "        self.output = self.activation_forward(self.input)\n",
    "        return self.activation_forward(self.input)\n",
    "    def backward(self,error):\n",
    "        return self.activation_backward(self.output)*error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f4b6e",
   "metadata": {},
   "source": [
    "## 7. Neural Network Engine\n",
    "The `Neural_Network` class serves as the container and trainer. It orchestrates the flow of tensors through the stack.\n",
    "\n",
    "**Training Loop Architecture:**\n",
    "1.  **Forward Propagation:** Sequential processing from Input $\\to$ Output.\n",
    "2.  **Loss Computation:** Evaluating model performance via MSE.\n",
    "3.  **Backpropagation:** Reverse-mode differentiation to propagate error gradients from Output $\\to$ Input.\n",
    "\n",
    "<br>\n",
    "\n",
    "### ðŸš€ Major Upgrades in V3.0:\n",
    "1.  **Vectorized Prediction (`Predict`):**\n",
    "    * **Old Way:** Looped through input samples one by one (slow).\n",
    "    * **New Way:** Processes the entire input matrix $X$ in a single operation using NumPy broadcasting.\n",
    "\n",
    "2.  **Mini-Batch Gradient Descent (`Training_model`):**\n",
    "    * **Stochastic Gradient Descent (SGD):** Updates weights after *every* sample. Noisy and slow in Python.\n",
    "    * **Mini-Batch:** Updates weights after a small batch (e.g., 10 samples). This leverages matrix multiplication speed and provides a more stable convergence.\n",
    "    * **Shuffling:** Data is shuffled every epoch to prevent the model from memorizing the order of samples.\n",
    "\n",
    "3.  **GUI Hooks (`callback`):**\n",
    "    * The training loop now accepts a `callback` function. This allows GUI to listen to progress, update progress bars, and send \"Stop\" signals safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d41909",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self):\n",
    "        self.layers=[]\n",
    "        self.loss=Activation.mse\n",
    "        self.delta_loss=Activation.del_mse\n",
    "        self.loss_history=[]\n",
    "        \n",
    "    def Add(self,user_layer):\n",
    "        self.layers.append(user_layer)\n",
    "\n",
    "    def Predict(self,input_data):\n",
    "        output = input_data\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def Training_model(self,x_train,y_train,epochs,callback=None,batch_size=32):\n",
    "        self.loss_history = []\n",
    "        samples = len(x_train)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            idx= np.arange(samples)\n",
    "            np.random.shuffle(idx)\n",
    "            x_train = x_train[idx]\n",
    "            y_train = y_train[idx]\n",
    "            for j in range(0,samples,batch_size):\n",
    "                x_batch=x_train[j:j+batch_size]\n",
    "                y_batch=y_train[j:j+batch_size]\n",
    "                output = x_batch\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward(output)\n",
    "                \n",
    "                y_true =y_batch\n",
    "                epoch_loss += self.loss(y_true, output)\n",
    "\n",
    "                error = self.delta_loss(y_true, output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward(error)\n",
    "                    \n",
    "            num_batches= samples/batch_size\n",
    "            epoch_loss/=num_batches\n",
    "            self.loss_history.append(epoch_loss)\n",
    "            \n",
    "            if callback is not None:\n",
    "                stop=callback(epoch,epoch_loss)\n",
    "                if stop is True:\n",
    "                    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
