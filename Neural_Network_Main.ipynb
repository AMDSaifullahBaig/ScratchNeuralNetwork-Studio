{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c0056e",
   "metadata": {},
   "source": [
    "# Modular Neural Network Main File\n",
    "**Author:** MD Saifullah Baig.A\n",
    "**Version:** 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c74dfc",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies\n",
    "We import standard libraries for mathematics (`numpy`) and visualization (`matplotlib`). \n",
    "Crucially, we import our custom `Neural_Network_Engine`, which contains the `Neural_Network`, `Connected_Layers`, and `Activation_Layer` classes built from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from Neural_Network_Engine import Neural_Network,Connected_Layers,Activation_Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea773a2d",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Helper Functions\n",
    "\n",
    "### Standard Scaler (Z-Score Normalization)\n",
    "Neural networks converge faster and more stably when input features are on a similar scale. This function normalizes the data to have a **Mean ($\\mu$) of 0** and a **Standard Deviation ($\\sigma$) of 1**.\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma + \\epsilon}$$\n",
    "\n",
    "The term $\\epsilon$ (set to `1e-8` in the code) is added for **Numerical Stability**.\n",
    "\n",
    "1.  **Prevents Division by Zero:**\n",
    "    * Standard Deviation ($\\sigma$) measures the spread of data.\n",
    "    * If a feature column contains **constant values** (e.g., every row has `Age = 25`), the variance and standard deviation will be **0**.\n",
    "    * Without $\\epsilon$, the computer would attempt to divide by zero ($\\frac{0}{0}$), causing the program to crash or resulting in `NaN` (Not a Number) or `Infinity`.\n",
    "\n",
    "2.  **Safety Net:**\n",
    "    * By adding a tiny number like $0.00000001$, the denominator becomes slightly larger than zero ($0 + \\epsilon$), allowing the calculation to proceed safely even on \"flat\" data features.\n",
    "\n",
    "**Returns:**\n",
    "* `scaled`: The normalized data.\n",
    "* `mean`, `std`: Stored to inverse-transform predictions later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d869ebd",
   "metadata": {},
   "source": [
    "### ❓ Why is Scaling Necessary?\n",
    "\n",
    "In Deep Learning, **Standard Scaling** is not optional—it is mathematically critical for the network to learn effectively. Here are the three main reasons why:\n",
    "\n",
    "#### 1. Prevents Feature Dominance (\"Apples vs. Oranges\")\n",
    "Neural networks use matrix multiplication (`Input * Weight`). If features have vastly different ranges, the larger numbers will dominate the learning process.\n",
    "* **Example:**\n",
    "    * *BMI:* Range 18–35\n",
    "    * *Income:* Range 20,000–100,000\n",
    "* **Result without Scaling:** The network sees \"Income\" as 1000x more important than \"BMI\" simply because the number is bigger. It effectively ignores the smaller feature.\n",
    "* **With Scaling:** Both features are forced into a similar range (approx. -3 to +3), giving them equal importance.\n",
    "\n",
    "#### 2. Faster Convergence (The \"Bowl\" Shape)\n",
    "The optimizer (Gradient Descent) tries to find the lowest error.\n",
    "* **Unscaled Data:** The error surface looks like a long, narrow valley. The optimizer zig-zags back and forth, taking a long time to reach the bottom.\n",
    "* **Scaled Data:** The error surface looks like a symmetrical bowl. The optimizer can take a direct path to the minimum, reducing training time significantly.\n",
    "\n",
    "#### 3. Avoids Vanishing Gradients (Activation Saturation)\n",
    "Activation functions like `Tanh` and `Sigmoid` are sensitive to large inputs.\n",
    "* **The Problem:** `Tanh(100)` is `1.0`. The slope (gradient) at this point is **Zero**.\n",
    "* **The Consequence:** If you feed raw large numbers (like 150) into the network, the gradients become zero immediately. The weights stop updating, and the network stops learning (Vanishing Gradient Problem).\n",
    "* **The Solution:** Scaling keeps inputs close to 0 (e.g., -1 to 1), where the activation functions have the steepest slope and strongest gradients.\n",
    "\n",
    "| Feature | Raw Data | Scaled Data |\n",
    "| :--- | :--- | :--- |\n",
    "| **Range** | Wildly different (e.g., 0.001 to 1,000,000) | Standardized (~ -3 to +3) |\n",
    "| **Training Speed** | Very Slow | Fast |\n",
    "| **Stability** | Prone to NaN / Infinity errors | Stable |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standard_Scaler(data):\n",
    "    mean=np.mean(data,axis=0)\n",
    "    std=np.std(data,axis=0)+1e-8\n",
    "    scaled=(data-mean)/std\n",
    "    return scaled,mean,std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b916ce",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "To evaluate the model fairly, we must test it on data it has never seen before.\n",
    "This function:\n",
    "1. Generates a list of indices.\n",
    "2. **Shuffles** them randomly to remove any ordering bias.\n",
    "3. Splits the data into **Training (80%)** and **Testing (20%)** sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X,Y,test_size=0.2):\n",
    "    idx=np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    split_range=int(X.shape[0]*(1-test_size))\n",
    "    train_idx,test_idx=idx[:split_range],idx[split_range:]\n",
    "    return X[train_idx],X[test_idx],Y[train_idx],Y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0184b54",
   "metadata": {},
   "source": [
    "## 3. Visualization\n",
    "This function generates two plots to evaluate performance:\n",
    "1. **Training Convergence:** Plots the MSE Loss over epochs (should decrease).\n",
    "2. **Prediction Accuracy:** A scatter plot comparing True values vs. Predicted values. A perfect model would align all points on the diagonal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848aa11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(loss_history,true,prediction):\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(loss_history,label=\"Training Loss\",color=\"blue\")\n",
    "    plt.title(\"Training Convergence\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.grid(True,linestyle=\"--\",alpha=0.6)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(true,prediction,alpha=0.6,color='red',edgecolors='k')\n",
    "    least=min(true.min(),prediction.min())\n",
    "    highest=max(true.max(),prediction.max())\n",
    "    plt.plot([least,highest],[least,highest],'k--',lw=2,label=\"Perfect Fit\")\n",
    "\n",
    "    plt.title(\"True VS Predicted Values\")\n",
    "    plt.xlabel(\"True labels\")\n",
    "    plt.ylabel(\"Predicted Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True,linestyle='--',alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1641451b",
   "metadata": {},
   "source": [
    "## 4. Main Execution Pipeline (`Build`)\n",
    "\n",
    "This block orchestrates the entire workflow:\n",
    "\n",
    "1.  **Load-Transform-Scale:** Loads the Diabetes dataset.\n",
    "2.  **Preprocessing:** Applies our custom `Standard_Scaler` to inputs ($X$) and targets ($y$).\n",
    "3.  **Splitting:** Divides data into Train/Test sets.\n",
    "4.  **Architecture:** Defines the Neural Network topology:\n",
    "    * Input Layer: 10 Features\n",
    "    * Hidden Layer 1: Fully Connected -> ReLU\n",
    "    * Hidden Layer 2: Fully Connected -> ReLU\n",
    "    * Output Layer: Fully Connected (Linear) -> Tanh (Optional)\n",
    "5.  **Training:** Optimizes weights using Backpropagation over 1000 epochs.\n",
    "6.  **Evaluation:** Predicts on test data and performs **Inverse Scaling** to get real-world values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build():\n",
    "    diabetes=load_diabetes()\n",
    "    X_raw=diabetes.data\n",
    "    y_raw=diabetes.target.reshape(-1, 1)\n",
    "    \n",
    "    X_scaled,mean_x,std_x=Standard_Scaler(X_raw)\n",
    "    y_scaled,mean_y,std_y=Standard_Scaler(y_raw)\n",
    "\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X_scaled,y_scaled,test_size=0.2)\n",
    "\n",
    "    model=Neural_Network()\n",
    "    \n",
    "    model.Add(Connected_Layers(10,5,learning_rate=0.01))\n",
    "    model.Add(Activation_Layer('relu'))\n",
    "    model.Add(Connected_Layers(5,1,learning_rate=0.01))\n",
    "    model.Add(Activation_Layer('tanh'))\n",
    "    \n",
    "    model.Training_model(X_train,y_train,epochs=100)\n",
    "    \n",
    "    preds_scaled=model.Predict(X_test)\n",
    "    preds_scaled=np.array(preds_scaled).reshape(-1, 1)\n",
    "    \n",
    "    preds_actual=(preds_scaled*std_y)+mean_y\n",
    "    y_test_actual=(y_test*std_y)+mean_y\n",
    "    \n",
    "    mse=Activation.mse(y_test_actual,preds_actual)\n",
    "\n",
    "    plot(model.loss_history,y_test_actual,preds_actual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74787ab8",
   "metadata": {},
   "source": [
    "### Run the Model\n",
    "Execute the function below to start training and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c4ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    Build()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
